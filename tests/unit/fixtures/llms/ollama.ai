# Local LLM configuration fixture  
llm "test_ollama" {
    provider: "ollama"
    model: "llama2"
    temperature: 0.5
    max_tokens: 200
}