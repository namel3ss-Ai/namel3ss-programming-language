# Test LLM configuration fixture
llm "test_openai" {
    provider: "openai"
    model: "gpt-3.5-turbo"
    temperature: 0.7
    max_tokens: 150
}