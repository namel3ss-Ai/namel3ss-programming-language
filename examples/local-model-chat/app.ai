# Local Model Chat Application
# Demonstrates local AI model deployment with vLLM, Ollama, and LocalAI

# =============================================================================
# Local Model Definitions
# =============================================================================

# Ollama model (easiest to start with)
ai model chat_model {
    provider: ollama
    model: llama3:8b
    config: {
        temperature: 0.8
        top_k: 40
        top_p: 0.9
        repeat_penalty: 1.1
        num_ctx: 4096
        max_tokens: 2048
    }
    deployment_config: {
        num_gpu: 1
        num_thread: 8
        keep_alive: "10m"
        host: "127.0.0.1"
        port: 11434
        auto_pull_model: true
        auto_start_server: true
    }
    description: "Conversational AI model via Ollama"
}

# vLLM model (production-grade performance)
ai model production_model {
    provider: vllm
    model: microsoft/DialoGPT-large
    config: {
        temperature: 0.7
        max_tokens: 1024
        top_p: 0.95
        frequency_penalty: 0.1
        presence_penalty: 0.1
    }
    deployment_config: {
        gpu_memory_utilization: 0.9
        max_model_len: 2048
        dtype: "float16"
        tensor_parallel_size: 1
        host: "127.0.0.1"
        port: 8001
        auto_start_server: true
    }
    description: "High-performance chat model via vLLM"
}

# LocalAI model (flexible format support)
ai model efficient_model {
    provider: local_ai
    model: efficient-chat
    local_model_path: "./models/efficient-chat.gguf"
    config: {
        temperature: 0.7
        max_tokens: 1024
        context_length: 2048
    }
    deployment_config: {
        backend: "llama-cpp"
        f16: true
        threads: 8
        gpu_layers: 35
        host: "127.0.0.1"
        port: 8002
        auto_start_server: true
        models_path: "./models"
    }
    description: "Efficient chat model via LocalAI"
}

# =============================================================================
# Chat Chain
# =============================================================================

chain chat_conversation {
    input: {
        message: str,
        model_name: str,
        conversation_history: list[object] = []
    }
    output: {
        response: str,
        model_used: str,
        response_time: float,
        updated_history: list[object]
    }
    
    step select_model {
        description: "Select the appropriate model based on request"
        if: "{{input.model_name == 'ollama'}}"
        model: chat_model
        prompt: |
            You are a helpful AI assistant. Please respond to the user's message naturally and helpfully.
            
            Conversation history:
            {% for msg in input.conversation_history %}
            {{msg.role}}: {{msg.content}}
            {% endfor %}
            
            User: {{input.message}}
            Assistant:
        output: response_text
    }
    
    step select_vllm_model {
        description: "Use vLLM model for production workloads"
        if: "{{input.model_name == 'vllm'}}"
        model: production_model
        output: response_text
        prompt: |
            You are a helpful AI assistant. Please respond to the user's message naturally and helpfully.
            
            Conversation history:
            {% for msg in input.conversation_history %}
            {{msg.role}}: {{msg.content}}
            {% endfor %}
            
            User: {{input.message}}
            Assistant:
    }
    
    step select_localai_model {
        description: "Use LocalAI model for efficient inference"
        if: "{{input.model_name == 'localai'}}"
        model: efficient_model
        prompt: |
            You are a helpful AI assistant. Please respond to the user's message naturally and helpfully.
            
            Conversation history:
            {% for msg in input.conversation_history %}
            {{msg.role}}: {{msg.content}}
            {% endfor %}
            
            User: {{input.message}}
            Assistant:
        output: response_text
    }
    
    step format_response {
        description: "Format the final response with metadata"
        python: |
            import time
            
            # Get response from whichever model was used
            response_text = (
                context.get('response_text') or
                step_outputs.get('select_model', {}).get('response_text') or
                step_outputs.get('select_vllm_model', {}).get('response_text') or 
                step_outputs.get('select_localai_model', {}).get('response_text') or
                "I'm sorry, I couldn't generate a response."
            )
            
            # Update conversation history
            updated_history = input.conversation_history.copy()
            updated_history.extend([
                {"role": "user", "content": input.message},
                {"role": "assistant", "content": response_text}
            ])
            
            # Keep only last 10 messages to manage memory
            if len(updated_history) > 10:
                updated_history = updated_history[-10:]
            
            return {
                "response": response_text,
                "model_used": input.model_name,
                "response_time": 0.0,  # Will be measured by frontend
                "updated_history": updated_history
            }
        output: {
            response: str,
            model_used: str,
            response_time: float,
            updated_history: list[object]
        }
    }
}

# =============================================================================
# Model Management Endpoints
# =============================================================================

endpoint GET /api/models {
    description: "List available local models and their status"
    response: {
        models: list[object]
    }
    
    python: |
        import asyncio
        from namel3ss.providers.factory import create_provider_from_spec
        
        models = []
        
        # Check Ollama model
        try:
            ollama_provider = create_provider_from_spec(
                "chat_ollama", "ollama", "llama3:8b",
                {"host": "127.0.0.1", "port": 11434}
            )
            ollama_health = await ollama_provider.health_check()
            models.append({
                "name": "chat_model",
                "provider": "ollama",
                "model": "llama3:8b",
                "status": ollama_health.get("status", "unknown"),
                "url": "http://127.0.0.1:11434",
                "health": ollama_health
            })
        except Exception as e:
            models.append({
                "name": "chat_model",
                "provider": "ollama", 
                "model": "llama3:8b",
                "status": "error",
                "error": str(e)
            })
        
        # Check vLLM model
        try:
            vllm_provider = create_provider_from_spec(
                "chat_vllm", "vllm", "microsoft/DialoGPT-large",
                {"host": "127.0.0.1", "port": 8001}
            )
            vllm_health = await vllm_provider.health_check()
            models.append({
                "name": "production_model",
                "provider": "vllm",
                "model": "microsoft/DialoGPT-large", 
                "status": vllm_health.get("status", "unknown"),
                "url": "http://127.0.0.1:8001",
                "health": vllm_health
            })
        except Exception as e:
            models.append({
                "name": "production_model",
                "provider": "vllm",
                "model": "microsoft/DialoGPT-large",
                "status": "error", 
                "error": str(e)
            })
        
        # Check LocalAI model
        try:
            localai_provider = create_provider_from_spec(
                "chat_localai", "local_ai", "efficient-chat",
                {"host": "127.0.0.1", "port": 8002}
            )
            localai_health = await localai_provider.health_check()
            models.append({
                "name": "efficient_model",
                "provider": "local_ai",
                "model": "efficient-chat",
                "status": localai_health.get("status", "unknown"),
                "url": "http://127.0.0.1:8002", 
                "health": localai_health
            })
        except Exception as e:
            models.append({
                "name": "efficient_model",
                "provider": "local_ai",
                "model": "efficient-chat",
                "status": "error",
                "error": str(e)
            })
        
        return {"models": models}
}

# =============================================================================
# Chat Endpoints
# =============================================================================

endpoint POST /api/chat {
    description: "Send a message to the selected model"
    request: {
        message: str,
        model: str = "ollama",
        history: list[object] = []
    }
    response: {
        response: str,
        model_used: str,
        response_time: float,
        history: list[object]
    }
    
    chain: chat_conversation
    input_mapping: {
        message: "{{request.message}}",
        model_name: "{{request.model}}",
        conversation_history: "{{request.history}}"
    }
    output_mapping: {
        response: "{{response.response}}",
        model_used: "{{response.model_used}}",
        response_time: "{{response.response_time}}", 
        history: "{{response.updated_history}}"
    }
}

endpoint POST /api/chat/stream {
    description: "Stream chat response from selected model"
    request: {
        message: str,
        model: str = "ollama", 
        history: list[object] = []
    }
    response_stream: {
        chunk: str,
        done: bool = false,
        model_used: str = "",
        metadata: object = {}
    }
    
    python: |
        import json
        import asyncio
        from namel3ss.providers.factory import create_provider_from_spec
        from namel3ss.providers.base import ProviderMessage
        
        async def stream_response():
            model_name = request.model
            
            # Prepare conversation context
            messages = []
            for msg in request.history:
                messages.append(ProviderMessage(
                    role=msg["role"],
                    content=msg["content"]
                ))
            messages.append(ProviderMessage(
                role="user", 
                content=request.message
            ))
            
            # Select provider based on model
            provider_configs = {
                "ollama": ("ollama", "llama3:8b", {"host": "127.0.0.1", "port": 11434}),
                "vllm": ("vllm", "microsoft/DialoGPT-large", {"host": "127.0.0.1", "port": 8001}),
                "localai": ("local_ai", "efficient-chat", {"host": "127.0.0.1", "port": 8002})
            }
            
            if model_name not in provider_configs:
                yield {
                    "chunk": f"Error: Unknown model '{model_name}'",
                    "done": True,
                    "model_used": model_name,
                    "metadata": {"error": "Invalid model selection"}
                }
                return
            
            provider_type, model_id, config = provider_configs[model_name]
            
            try:
                provider = create_provider_from_spec(
                    f"stream_{model_name}", provider_type, model_id, config
                )
                
                full_response = ""
                async for chunk in provider.stream(messages):
                    content = chunk.content
                    full_response += content
                    
                    yield {
                        "chunk": content,
                        "done": False,
                        "model_used": model_name,
                        "metadata": chunk.metadata
                    }
                
                # Send final chunk
                yield {
                    "chunk": "",
                    "done": True,
                    "model_used": model_name,
                    "metadata": {"full_response": full_response}
                }
                
            except Exception as e:
                yield {
                    "chunk": f"Error: {str(e)}",
                    "done": True,
                    "model_used": model_name,
                    "metadata": {"error": str(e)}
                }
        
        return stream_response()
}

# =============================================================================
# Model Management Endpoints  
# =============================================================================

endpoint POST /api/models/{model_name}/start {
    description: "Start a local model deployment"
    path_params: {
        model_name: str
    }
    response: {
        status: str,
        message: str
    }
    
    python: |
        import subprocess
        import sys
        
        try:
            # Use namel3ss CLI to start deployment
            result = subprocess.run([
                sys.executable, "-m", "namel3ss", 
                "deploy", "local", "start", model_name
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                return {
                    "status": "success",
                    "message": f"Model {model_name} started successfully"
                }
            else:
                return {
                    "status": "error",
                    "message": f"Failed to start model: {result.stderr}"
                }
        except Exception as e:
            return {
                "status": "error",
                "message": f"Error starting model: {str(e)}"
            }
}

endpoint POST /api/models/{model_name}/stop {
    description: "Stop a local model deployment"
    path_params: {
        model_name: str
    }
    response: {
        status: str,
        message: str
    }
    
    python: |
        import subprocess
        import sys
        
        try:
            # Use namel3ss CLI to stop deployment
            result = subprocess.run([
                sys.executable, "-m", "namel3ss",
                "deploy", "local", "stop", model_name
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                return {
                    "status": "success", 
                    "message": f"Model {model_name} stopped successfully"
                }
            else:
                return {
                    "status": "error",
                    "message": f"Failed to stop model: {result.stderr}"
                }
        except Exception as e:
            return {
                "status": "error",
                "message": f"Error stopping model: {str(e)}"
            }
}

# =============================================================================
# Frontend Page
# =============================================================================

page ChatApp {
    title: "Local Model Chat"
    description: "Chat with locally deployed AI models"
    
    layout: |
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Local Model Chat</title>
            <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
            <script src="https://unpkg.com/axios/dist/axios.min.js"></script>
            <style>
                body { 
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
                    margin: 0; padding: 20px; 
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: #333;
                    min-height: 100vh;
                }
                .container { 
                    max-width: 1200px; 
                    margin: 0 auto; 
                    background: white;
                    border-radius: 15px;
                    box-shadow: 0 20px 40px rgba(0,0,0,0.1);
                    overflow: hidden;
                }
                .header {
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 20px;
                    text-align: center;
                }
                .header h1 {
                    margin: 0;
                    font-size: 2.5em;
                }
                .header p {
                    margin: 10px 0 0 0;
                    opacity: 0.9;
                }
                .chat-container {
                    display: flex;
                    height: 600px;
                }
                .sidebar {
                    width: 300px;
                    background: #f8f9fa;
                    border-right: 1px solid #e9ecef;
                    padding: 20px;
                }
                .model-selector {
                    margin-bottom: 20px;
                }
                .model-selector h3 {
                    margin: 0 0 10px 0;
                    color: #495057;
                }
                .model-option {
                    display: flex;
                    align-items: center;
                    margin-bottom: 10px;
                    padding: 10px;
                    border: 2px solid #e9ecef;
                    border-radius: 8px;
                    cursor: pointer;
                    transition: all 0.2s;
                }
                .model-option:hover {
                    border-color: #667eea;
                    background: #f8f9ff;
                }
                .model-option.active {
                    border-color: #667eea;
                    background: linear-gradient(135deg, #667eea, #764ba2);
                    color: white;
                }
                .model-status {
                    margin-left: auto;
                    font-size: 12px;
                    padding: 2px 6px;
                    border-radius: 10px;
                }
                .status-healthy { background: #28a745; color: white; }
                .status-error { background: #dc3545; color: white; }
                .status-unknown { background: #6c757d; color: white; }
                .chat-area {
                    flex: 1;
                    display: flex;
                    flex-direction: column;
                }
                .messages {
                    flex: 1;
                    overflow-y: auto;
                    padding: 20px;
                    background: #ffffff;
                }
                .message {
                    margin-bottom: 20px;
                    display: flex;
                    flex-direction: column;
                }
                .message.user {
                    align-items: flex-end;
                }
                .message.assistant {
                    align-items: flex-start;
                }
                .message-content {
                    max-width: 70%;
                    padding: 15px 20px;
                    border-radius: 20px;
                    line-height: 1.4;
                }
                .message.user .message-content {
                    background: linear-gradient(135deg, #667eea, #764ba2);
                    color: white;
                }
                .message.assistant .message-content {
                    background: #f1f3f4;
                    color: #333;
                }
                .input-area {
                    padding: 20px;
                    border-top: 1px solid #e9ecef;
                    background: #f8f9fa;
                }
                .input-form {
                    display: flex;
                    gap: 10px;
                }
                .message-input {
                    flex: 1;
                    padding: 15px;
                    border: 2px solid #e9ecef;
                    border-radius: 25px;
                    outline: none;
                    font-size: 16px;
                    transition: border-color 0.2s;
                }
                .message-input:focus {
                    border-color: #667eea;
                }
                .send-button {
                    padding: 15px 30px;
                    background: linear-gradient(135deg, #667eea, #764ba2);
                    color: white;
                    border: none;
                    border-radius: 25px;
                    cursor: pointer;
                    font-size: 16px;
                    font-weight: 600;
                    transition: transform 0.2s;
                }
                .send-button:hover {
                    transform: translateY(-2px);
                }
                .send-button:disabled {
                    opacity: 0.6;
                    cursor: not-allowed;
                    transform: none;
                }
                .loading {
                    text-align: center;
                    color: #6c757d;
                    padding: 20px;
                }
                .model-controls {
                    margin-top: 20px;
                    padding-top: 20px;
                    border-top: 1px solid #e9ecef;
                }
                .control-button {
                    width: 100%;
                    padding: 10px;
                    margin-bottom: 5px;
                    border: 1px solid #667eea;
                    background: white;
                    color: #667eea;
                    border-radius: 5px;
                    cursor: pointer;
                    transition: all 0.2s;
                }
                .control-button:hover {
                    background: #667eea;
                    color: white;
                }
            </style>
        </head>
        <body>
            <div id="app">
                <div class="container">
                    <div class="header">
                        <h1>Local Model Chat</h1>
                        <p>Chat with locally deployed AI models using vLLM, Ollama, and LocalAI</p>
                    </div>
                    
                    <div class="chat-container">
                        <div class="sidebar">
                            <div class="model-selector">
                                <h3>Select Model</h3>
                                <div v-for="model in availableModels" :key="model.name"
                                     class="model-option"
                                     :class="{ active: selectedModel === model.provider }"
                                     @click="selectModel(model.provider)">
                                    <div>
                                        <div style="font-weight: 600;">{{ model.provider }}</div>
                                        <div style="font-size: 12px; opacity: 0.7;">{{ model.model }}</div>
                                    </div>
                                    <span class="model-status"
                                          :class="'status-' + (model.status || 'unknown')">
                                        {{ model.status || 'unknown' }}
                                    </span>
                                </div>
                            </div>
                            
                            <div class="model-controls">
                                <h4>Model Controls</h4>
                                <button class="control-button" @click="refreshModels">
                                    Refresh Status
                                </button>
                                <button class="control-button" @click="startModel">
                                    Start Model
                                </button>
                                <button class="control-button" @click="stopModel">
                                    Stop Model
                                </button>
                            </div>
                        </div>
                        
                        <div class="chat-area">
                            <div class="messages" ref="messages">
                                <div v-if="messages.length === 0" class="loading">
                                    Select a model and start chatting!
                                </div>
                                <div v-for="message in messages" :key="message.id" 
                                     class="message" :class="message.role">
                                    <div class="message-content">{{ message.content }}</div>
                                </div>
                                <div v-if="isLoading" class="loading">
                                    Model is thinking...
                                </div>
                            </div>
                            
                            <div class="input-area">
                                <div class="input-form">
                                    <input v-model="currentMessage" 
                                           @keyup.enter="sendMessage"
                                           class="message-input"
                                           placeholder="Type your message here..."
                                           :disabled="isLoading">
                                    <button @click="sendMessage" 
                                            class="send-button"
                                            :disabled="isLoading || !currentMessage.trim()">
                                        Send
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <script>
                const { createApp } = Vue;
                
                createApp({
                    data() {
                        return {
                            currentMessage: '',
                            messages: [],
                            selectedModel: 'ollama',
                            isLoading: false,
                            availableModels: [],
                            messageId: 0
                        };
                    },
                    
                    async mounted() {
                        await this.loadModels();
                    },
                    
                    methods: {
                        async loadModels() {
                            try {
                                const response = await axios.get('/api/models');
                                this.availableModels = response.data.models;
                            } catch (error) {
                                console.error('Error loading models:', error);
                            }
                        },
                        
                        selectModel(provider) {
                            this.selectedModel = provider;
                        },
                        
                        async sendMessage() {
                            if (!this.currentMessage.trim() || this.isLoading) return;
                            
                            const userMessage = {
                                id: ++this.messageId,
                                role: 'user',
                                content: this.currentMessage
                            };
                            
                            this.messages.push(userMessage);
                            const messageText = this.currentMessage;
                            this.currentMessage = '';
                            this.isLoading = true;
                            
                            try {
                                const response = await axios.post('/api/chat', {
                                    message: messageText,
                                    model: this.selectedModel,
                                    history: this.messages.slice(0, -1).map(m => ({
                                        role: m.role,
                                        content: m.content
                                    }))
                                });
                                
                                const assistantMessage = {
                                    id: ++this.messageId,
                                    role: 'assistant',
                                    content: response.data.response
                                };
                                
                                this.messages.push(assistantMessage);
                                
                            } catch (error) {
                                const errorMessage = {
                                    id: ++this.messageId,
                                    role: 'assistant',
                                    content: `Error: ${error.response?.data?.detail || error.message}`
                                };
                                this.messages.push(errorMessage);
                            } finally {
                                this.isLoading = false;
                                this.$nextTick(() => {
                                    this.scrollToBottom();
                                });
                            }
                        },
                        
                        async refreshModels() {
                            await this.loadModels();
                        },
                        
                        async startModel() {
                            const modelName = this.getModelName(this.selectedModel);
                            if (!modelName) return;
                            
                            try {
                                await axios.post(`/api/models/${modelName}/start`);
                                await this.loadModels();
                            } catch (error) {
                                alert(`Error starting model: ${error.message}`);
                            }
                        },
                        
                        async stopModel() {
                            const modelName = this.getModelName(this.selectedModel);
                            if (!modelName) return;
                            
                            try {
                                await axios.post(`/api/models/${modelName}/stop`);
                                await this.loadModels();
                            } catch (error) {
                                alert(`Error stopping model: ${error.message}`);
                            }
                        },
                        
                        getModelName(provider) {
                            const modelMap = {
                                'ollama': 'chat_model',
                                'vllm': 'production_model',
                                'localai': 'efficient_model'
                            };
                            return modelMap[provider];
                        },
                        
                        scrollToBottom() {
                            const messagesEl = this.$refs.messages;
                            messagesEl.scrollTop = messagesEl.scrollHeight;
                        }
                    }
                }).mount('#app');
            </script>
        </body>
        </html>
}