// Test complete RAG pipeline with validation

app "Documentation Assistant"

// Define LLM for responses
llm gpt4:
    provider: openai
    model: gpt-4
    temperature: 0.7
    max_tokens: 2000

// Define vector index
index docs_index:
    source_dataset: documentation
    embedding_model: text-embedding-3-small
    chunk_size: 512
    overlap: 64
    backend: pgvector
    table_name: doc_embeddings

// Define RAG pipeline
rag_pipeline doc_retrieval:
    query_encoder: text-embedding-3-small
    index: docs_index
    top_k: 5
    distance_metric: cosine

// Define prompt template that uses retrieved docs
prompt doc_qa:
    input:
        - question: text (required)
        - context: text (required)
    
    template: |
        You are a helpful documentation assistant.
        
        Context from documentation:
        {context}
        
        Question: {question}
        
        Provide a clear, accurate answer based on the context above.
    
    output:
        - answer: text

// Define chain that combines RAG retrieval with LLM response
chain rag_qa_chain:
    input -> rag doc_retrieval -> prompt doc_qa -> llm gpt4

